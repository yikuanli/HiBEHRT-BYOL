{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.insert(0, '../')\n",
    "from CPRD.spark import spark_init,read_txt,read_parquet, read_csv\n",
    "from CPRD.table import Patient,Practice,Clinical,Diagnosis,Hes, Therapy, cvt_str2time, EHR\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import *\n",
    "import random\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from utils.utils import save_obj, load_obj\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(code_cnt, symbols=None):\n",
    "    if symbols is None:\n",
    "        symbols = [\"PAD\", \"UNK\", \"SEP\", \"CLS\", \"MASK\"]\n",
    "\n",
    "    # initialize dictionaries\n",
    "    token2idx = {}\n",
    "    idx2token = {}\n",
    "\n",
    "    # set up predefined symbols\n",
    "    for i in range(len(symbols)):\n",
    "        token2idx[str(symbols[i])] = i\n",
    "        idx2token[i] = str(symbols[i])\n",
    "\n",
    "    # add all the tokens into the dictionary\n",
    "    token = code_cnt.code.values\n",
    "    for i in range(len(token)):\n",
    "        idx = i + len(symbols)\n",
    "        token2idx[str(token[i])] = idx\n",
    "        idx2token[idx] = str(token[i])\n",
    "\n",
    "    return token2idx, idx2token\n",
    "\n",
    "def age_vocab(max_age, mon=1, symbol=None):\n",
    "    age2idx = {}\n",
    "    idx2age = {}\n",
    "    if symbol is None:\n",
    "        symbol = ['PAD', 'UNK']\n",
    "\n",
    "    for i in range(len(symbol)):\n",
    "        age2idx[str(symbol[i])] = i\n",
    "        idx2age[i] = str(symbol[i])\n",
    "\n",
    "    if mon == 12:\n",
    "        for i in range(max_age):\n",
    "            age2idx[str(i)] = len(symbol) + i\n",
    "            idx2age[len(symbol) + i] = str(i)\n",
    "    elif mon == 1:\n",
    "        for i in range(max_age * 12):\n",
    "            age2idx[str(i)] = len(symbol) + i\n",
    "            idx2age[len(symbol) + i] = str(i)\n",
    "    else:\n",
    "        age2idx = None\n",
    "        idx2age = None\n",
    "    return age2idx, idx2age\n",
    "\n",
    "age2idx, idx2age = age_vocab(max_age=110, mon=12, symbol=None)\n",
    "\n",
    "data = {\n",
    "    'token2idx': age2idx,\n",
    "    'idx2token': idx2age\n",
    "}\n",
    "\n",
    "save_obj(data, '/home/shared/yikuan/HiBEHRT/data/dict4age')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_col(df, old, new):\n",
    "    \"\"\"rename pyspark dataframe column\"\"\"\n",
    "    return df.withColumnRenamed(old, new)\n",
    "\n",
    "# construct representation learning dataset \n",
    "def check_time(df, col, time_a=1985, time_b=2015):\n",
    "    \"\"\"keep data with date between a and b\"\"\"\n",
    "    year = F.udf(lambda x: x.year)\n",
    "    df = df.withColumn('Y', year(col))\n",
    "    df = df.filter(F.col('Y') >= time_a)\n",
    "    df = df.filter(F.col('Y') <= time_b).drop('Y')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = spark_init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read representation list \n",
    "rep_pat = read_parquet(spark.sqlContext, '/home/shared/yikuan/HiBEHRT/data/pat_rep.parquet')\n",
    "rep_pat = rename_col(rep_pat, 'patid', 'patid_eligible')\n",
    "\n",
    "# diagnose\n",
    "diag = read_parquet(spark.sqlContext, '/home/shared/yikuan/HiBEHRT/data/diagnoses.parquet')\n",
    "diag = check_time(diag, 'eventdate', time_a=1985, time_b=2005)\n",
    "diag = diag.join(rep_pat, diag.patid==rep_pat.patid_eligible, 'left')\n",
    "diag = diag.filter(F.col('patid_eligible').isNotNull()).drop('patid_eligible')\n",
    "\n",
    "def select_diagnose(diagnoses):\n",
    "    diagnoses_icd = diagnoses.filter(F.col('type')=='icd')\\\n",
    "                    .withColumn('first', F.col('code').substr(0,1))\\\n",
    "                    .filter(F.col('first').isin(*['Z','V','R','U','X','Y']) == False)\\\n",
    "                    .select(['patid', 'eventdate', 'code', 'type', 'source'])\n",
    "    diagnoses_read = diagnoses.filter(F.col('type')=='read')\\\n",
    "                .withColumn('first', F.col('code').substr(0,1))\\\n",
    "                .filter(F.col('first').isin(*['0', '1', '2', '3', '4', '5', '6', '7','8', '9', 'Z', 'U']) == False)\\\n",
    "                .select(['patid', 'eventdate', 'code', 'type', 'source'])\n",
    "    diagnoses = diagnoses_icd.union(diagnoses_read)    \n",
    "    diagnoses = diagnoses.select(['patid', 'eventdate', 'code'])\n",
    "    return diagnoses\n",
    "\n",
    "diag = select_diagnose(diag) # patid, event date, code\n",
    "\n",
    "print('diag:',diag.schema)\n",
    "\n",
    "# medication\n",
    "med = read_parquet(spark.sqlContext, '/home/shared/yikuan/HiBEHRT/data/medication.parquet/')\n",
    "med = check_time(med, 'eventdate', time_a=1985, time_b=2005)\n",
    "med = med.join(rep_pat, med.patid==rep_pat.patid_eligible, 'left')\n",
    "med = med.filter(F.col('patid_eligible').isNotNull()).drop('patid_eligible') # patid, event date, code\n",
    "\n",
    "print('med:',med.schema)\n",
    "\n",
    "# hes procedure\n",
    "procedure = read_parquet(spark.sqlContext, '/home/shared/yikuan/HiBEHRT/data/hes_procedure.parquet/')\n",
    "procedure = check_time(procedure, 'eventdate', time_a=1985, time_b=2005)\n",
    "procedure = procedure.join(rep_pat, procedure.patid==rep_pat.patid_eligible, 'left')\n",
    "procedure = procedure.filter(F.col('patid_eligible').isNotNull()).drop('patid_eligible')\n",
    "procedure = rename_col(procedure, 'OPCS', 'code') # patid, event date, code\n",
    "\n",
    "print('procedure:',procedure.schema)\n",
    "\n",
    "# cprd test\n",
    "test = read_parquet(spark.sqlContext, '/home/shared/yikuan/HiBEHRT/data/cprd_test.parquet/')\n",
    "test = check_time(test, 'eventdate', time_a=1985, time_b=2005)\n",
    "test = test.join(rep_pat, test.patid==rep_pat.patid_eligible, 'left')\n",
    "test = test.filter(F.col('patid_eligible').isNotNull()).drop('patid_eligible').select(['patid', 'eventdate', 'enttype'])\n",
    "test = rename_col(test, 'enttype', 'code')\n",
    "\n",
    "print('test:',test.schema)\n",
    "\n",
    "# bmi\n",
    "bmi = read_parquet(spark.sqlContext, '/home/shared/yikuan/HiBEHRT/data/bmi.parquet/')\n",
    "bmi = check_time(bmi, 'eventdate', time_a=1985, time_b=2005)\n",
    "bmi = bmi.join(rep_pat, bmi.patid==rep_pat.patid_eligible, 'left')\n",
    "bmi = bmi.filter(F.col('patid_eligible').isNotNull()).drop('patid_eligible')\n",
    "\n",
    "round_bmi = F.udf(lambda x: int(x//1))\n",
    "bmi = bmi.withColumn('BMI', round_bmi('BMI'))\n",
    "bmi = rename_col(bmi, 'BMI', 'code')\n",
    "\n",
    "print('bmi:',bmi.schema)\n",
    "\n",
    "# bp_low\n",
    "bp_low = read_parquet(spark.sqlContext, '/home/shared/yikuan/HiBEHRT/data/bp_low.parquet/')\n",
    "bp_low = check_time(bp_low, 'eventdate', time_a=1985, time_b=2005)\n",
    "bp_low = bp_low.join(rep_pat, bp_low.patid==rep_pat.patid_eligible, 'left')\n",
    "bp_low = bp_low.filter(F.col('patid_eligible').isNotNull()).drop('patid_eligible')\n",
    "\n",
    "round_bp = F.udf(lambda x: int(x//5))\n",
    "bp_low = bp_low.withColumn('bp_low', round_bp('bp_low'))\n",
    "bp_low = rename_col(bp_low, 'bp_low', 'code')\n",
    "\n",
    "print('bp_low:',bp_low.schema)\n",
    "\n",
    "# bp_high\n",
    "bp_high = read_parquet(spark.sqlContext, '/home/shared/yikuan/HiBEHRT/data/bp_high.parquet/')\n",
    "bp_high = check_time(bp_high, 'eventdate', time_a=1985, time_b=2005)\n",
    "bp_high = bp_high.join(rep_pat, bp_high.patid==rep_pat.patid_eligible, 'left')\n",
    "bp_high = bp_high.filter(F.col('patid_eligible').isNotNull()).drop('patid_eligible')\n",
    "\n",
    "round_bp = F.udf(lambda x: int(x//5))\n",
    "bp_high = bp_high.withColumn('bp_high', round_bp('bp_high'))\n",
    "bp_high = rename_col(bp_high, 'bp_high', 'code')\n",
    "\n",
    "print('bp_high:', bp_high.schema)\n",
    "\n",
    "# smoke\n",
    "smoke = read_parquet(spark.sqlContext, '/home/shared/yikuan/HiBEHRT/data/smoke.parquet/')\n",
    "smoke = check_time(smoke, 'eventdate', time_a=1985, time_b=2005)\n",
    "smoke = smoke.join(rep_pat, smoke.patid==rep_pat.patid_eligible, 'left')\n",
    "smoke = smoke.filter(F.col('patid_eligible').isNotNull()).drop('patid_eligible')\n",
    "smoke = rename_col(smoke, 'smoke', 'code')\n",
    "\n",
    "print('smoke:',smoke.schema)\n",
    "\n",
    "# alcohol\n",
    "alcohol = read_parquet(spark.sqlContext, '/home/shared/yikuan/HiBEHRT/data/alcohol.parquet/')\n",
    "alcohol = check_time(alcohol, 'eventdate', time_a=1985, time_b=2005)\n",
    "alcohol = alcohol.join(rep_pat, alcohol.patid==rep_pat.patid_eligible, 'left')\n",
    "alcohol = alcohol.filter(F.col('patid_eligible').isNotNull()).drop('patid_eligible')\n",
    "alcohol = rename_col(alcohol, 'alcohol', 'code')\n",
    "\n",
    "print('alcohol:',alcohol.schema)\n",
    "\n",
    "# add signature for each mordality\n",
    "diag = diag.withColumn('code', F.concat(F.lit('DIA'), diag['code']))\n",
    "med = med.withColumn('code', F.concat(F.lit('MED'), med['code']))\n",
    "procedure = procedure.withColumn('code', F.concat(F.lit('PRO'), procedure['code']))\n",
    "test = test.withColumn('code', F.concat(F.lit('TES'), test['code']))\n",
    "bmi = bmi.withColumn('code', F.concat(F.lit('BMI'), bmi['code']))\n",
    "bp_low = bp_low.withColumn('code', F.concat(F.lit('BPL'), bp_low['code']))\n",
    "bp_high = bp_high.withColumn('code', F.concat(F.lit('BPH'), bp_high['code']))\n",
    "smoke = smoke.withColumn('code', F.concat(F.lit('SMO'), smoke['code']))\n",
    "alcohol = alcohol.withColumn('code', F.concat(F.lit('ALC'), alcohol['code']))\n",
    "\n",
    "def format_age(path_to_demo, data):\n",
    "    demographic = Patient(read_txt(spark.sc, spark.sqlContext, path=path_to_demo)) \\\n",
    "    .accept_flag().yob_calibration().cvt_crd2date().cvt_tod2date().cvt_deathdate2date().get_pracid().drop('accept')\\\n",
    "    .select(['patid', 'yob'])\n",
    "    \n",
    "    data = data.join(demographic, data.patid==demographic.patid, 'left').drop(demographic.patid)\n",
    "    data= EHR(data).cal_age('eventdate', 'yob', year=True, name='age').select(['patid', 'eventdate', 'code', 'age'])\n",
    "    return data\n",
    "\n",
    "diag = format_age('/home/workspace/datasets/cprd/cuts/02_cprd2015/1_RawData/Patient', diag)\n",
    "med = format_age('/home/workspace/datasets/cprd/cuts/02_cprd2015/1_RawData/Patient', med)\n",
    "procedure = format_age('/home/workspace/datasets/cprd/cuts/02_cprd2015/1_RawData/Patient', procedure)\n",
    "test = format_age('/home/workspace/datasets/cprd/cuts/02_cprd2015/1_RawData/Patient', test)\n",
    "bmi = format_age('/home/workspace/datasets/cprd/cuts/02_cprd2015/1_RawData/Patient', bmi)\n",
    "bp_low = format_age('/home/workspace/datasets/cprd/cuts/02_cprd2015/1_RawData/Patient', bp_low)\n",
    "bp_high = format_age('/home/workspace/datasets/cprd/cuts/02_cprd2015/1_RawData/Patient', bp_high)\n",
    "smoke = format_age('/home/workspace/datasets/cprd/cuts/02_cprd2015/1_RawData/Patient', smoke)\n",
    "alcohol = format_age('/home/workspace/datasets/cprd/cuts/02_cprd2015/1_RawData/Patient', alcohol)\n",
    "\n",
    "# ensemble\n",
    "records = diag.union(med).union(procedure).union(test).union(bmi).union(bp_low).union(bp_high).union(smoke).union(alcohol)\n",
    "\n",
    "# # create dictionary\n",
    "# encounter = records.groupBy('code').count().toPandas()\n",
    "# encounter = encounter[encounter['count']>1000]\n",
    "\n",
    "# def create_vocab(code_cnt, symbols=None):\n",
    "#     if symbols is None:\n",
    "#         symbols = [\"PAD\", \"UNK\", \"SEP\", \"CLS\", \"MASK\"]\n",
    "\n",
    "#     # initialize dictionaries\n",
    "#     token2idx = {}\n",
    "#     idx2token = {}\n",
    "\n",
    "#     # set up predefined symbols\n",
    "#     for i in range(len(symbols)):\n",
    "#         token2idx[str(symbols[i])] = i\n",
    "#         idx2token[i] = str(symbols[i])\n",
    "\n",
    "#     # add all the tokens into the dictionary\n",
    "#     token = code_cnt.code.values\n",
    "#     for i in range(len(token)):\n",
    "#         idx = i + len(symbols)\n",
    "#         token2idx[str(token[i])] = idx\n",
    "#         idx2token[idx] = str(token[i])\n",
    "\n",
    "#     return token2idx, idx2token\n",
    "\n",
    "# token2idx, idx2token = create_vocab(encounter)\n",
    "# # save vocab\n",
    "# data = {\n",
    "#     'token2idx': token2idx,\n",
    "#     'idx2token': idx2token\n",
    "# }\n",
    "\n",
    "# save_obj(data, '/home/shared/yikuan/HiBEHRT/data/dict4all')\n",
    "\n",
    "# del encounter\n",
    "# del data\n",
    "\n",
    "def format_sequence(data):\n",
    "    # group by date\n",
    "    data = data.groupby(['patid', 'eventdate']).agg(F.collect_list('code').alias('code'), F.collect_list('age').alias('age'))\n",
    "    \n",
    "    data = EHR(data).array_add_element('code', 'SEP')\n",
    "    # add extra age to fill the gap of sep\n",
    "    extract_age = F.udf(lambda x: x[0])\n",
    "    data = data.withColumn('age_temp', extract_age('age')).withColumn('age', F.concat(F.col('age'),F.array(F.col('age_temp')))).drop('age_temp')\n",
    "    \n",
    "    # sort and merge code and age\n",
    "    w = Window.partitionBy('patid').orderBy('eventdate')\n",
    "    data = data.withColumn('code',F.collect_list('code').over(w))\\\n",
    "                .withColumn('age', F.collect_list('age').over(w))\\\n",
    "                .groupBy('patid').agg(F.max('code').alias('code'), F.max('age').alias('age'))\n",
    "    data = EHR(data).array_flatten('code').array_flatten('age') # patid, code, age\n",
    "    return data\n",
    "\n",
    "records = format_sequence(records)\n",
    " \n",
    "def format_seg_and_position_code(data):\n",
    "#     \n",
    "    def seg_records(x):\n",
    "        seg_list = []\n",
    "        flag = 0\n",
    "        for each in x:\n",
    "            if each != 'SEP':\n",
    "                seg_list.append(flag)\n",
    "            else:\n",
    "                seg_list.append(flag)\n",
    "                flag = (flag + 1)%2\n",
    "                \n",
    "        return seg_list\n",
    "                \n",
    "    \n",
    "    seg = F.udf(lambda x: seg_records(x),ArrayType(StringType(),True))\n",
    "    data = data.withColumn('seg', seg('code'))\n",
    "    \n",
    "    def posi_records(x):\n",
    "        posi_list = []\n",
    "        flag = 0\n",
    "        for each in x:\n",
    "            if each != 'SEP':\n",
    "                posi_list.append(flag)\n",
    "            else:\n",
    "                posi_list.append(flag)\n",
    "                flag = flag + 1\n",
    "        return posi_list\n",
    "    \n",
    "    posi = F.udf(lambda x: posi_records(x),ArrayType(StringType(),True))\n",
    "    data = data.withColumn('position', posi('code'))\n",
    "    \n",
    "    return data\n",
    "\n",
    "records = format_seg_and_position_code(records)\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([StructField('patid', StringType(), True),\n",
    "                     StructField('code',ArrayType(StringType(),True), True),\n",
    "                     StructField('age', ArrayType(StringType(),True), True),\n",
    "                     StructField('seg', ArrayType(StringType(),True), True),\n",
    "                     StructField('position', ArrayType(StringType(),True), True)\n",
    "                     ])\n",
    "\n",
    "def remove_sep(patid, code, age, seg, position):\n",
    "    code_list = []\n",
    "    age_list = []\n",
    "    seg_list = []\n",
    "    position_list = []\n",
    "    \n",
    "    for i in range(len(code)):\n",
    "        if code[i]!='SEP':\n",
    "            code_list.append(code[i])\n",
    "            age_list.append(age[i])\n",
    "            seg_list.append(seg[i])\n",
    "            position_list.append(position[i])\n",
    "    return patid, code, age, seg, position\n",
    "        \n",
    "\n",
    "test_udf = F.udf(remove_sep, schema)\n",
    "records = records.select(test_udf('patid', 'code', 'age', 'seg', 'position').alias(\"test\"))\n",
    "records = records.select(\"test.*\")\n",
    "\n",
    "\n",
    "records.write.parquet('/home/shared/yikuan/HiBEHRT/data/selfsupervise.parquet')\n",
    "print('end')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# merge patient from 1985 - 2015, exclude pateint who are positive within 1985-2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_summary_1985_2005 = read_parquet(spark.sqlContext, '/home/shared/yikuan/HiBEHRT/data/HF/HF_1985_2005.parquet/')\n",
    "patient_summary_2005_2015 = read_parquet(spark.sqlContext, '/home/shared/yikuan/HiBEHRT/data/HF/HF_2005_2015.parquet/')\n",
    "\n",
    "name_list = ['patid', 'firstDate', 'lastDate', 'label', 'HFDate']\n",
    "for name in name_list:\n",
    "    patient_summary_1985_2005 = rename_col(patient_summary_1985_2005, name, '{}_1985_2005'.format(name))\n",
    "    patient_summary_2005_2015 = rename_col(patient_summary_2005_2015, name, '{}_2005_2015'.format(name))\n",
    "\n",
    "patient_summary = patient_summary_2005_2015.join(patient_summary_1985_2005, patient_summary_2005_2015.patid_2005_2015==patient_summary_1985_2005.patid_1985_2005, 'left')\n",
    "\n",
    "# paitent doesnt exist in 1985-2005 use all information from 2005 - 2015\n",
    "patient_summary_new = patient_summary.filter(F.col('patid_1985_2005').isNull())\n",
    "patient_summary_new = rename_col(patient_summary_new, 'patid_2005_2015', 'patid')\n",
    "patient_summary_new = rename_col(patient_summary_new, 'firstDate_2005_2015', 'firstDate')\n",
    "patient_summary_new = rename_col(patient_summary_new, 'lastDate_2005_2015', 'lastDate')\n",
    "patient_summary_new = rename_col(patient_summary_new, 'label_2005_2015', 'label')\n",
    "patient_summary_new = rename_col(patient_summary_new, 'HFDate_2005_2015', 'HFDate')\n",
    "patient_summary_new = patient_summary_new.select(name_list)\n",
    "\n",
    "# process patient exist in 1985-2005\n",
    "# 1. remove patient who are (+) in 1985-2005\n",
    "patient_summary_old = patient_summary.filter(F.col('patid_1985_2005').isNotNull()).filter(F.col('label_1985_2005')!=1)\n",
    "# 2. keep patient_2005_2015, first date 1985-2005,  last date 2005-2015, label 2005-2015, HF date 2005-2015\n",
    "\n",
    "patient_summary_old = rename_col(patient_summary_old, 'patid_2005_2015', 'patid')\n",
    "patient_summary_old = rename_col(patient_summary_old, 'firstDate_1985_2005', 'firstDate')\n",
    "patient_summary_old = rename_col(patient_summary_old, 'lastDate_2005_2015', 'lastDate')\n",
    "patient_summary_old = rename_col(patient_summary_old, 'label_2005_2015', 'label')\n",
    "patient_summary_old = rename_col(patient_summary_old, 'HFDate_2005_2015', 'HFDate')\n",
    "patient_summary_old = patient_summary_old.select(name_list)\n",
    "\n",
    "patient_summary = patient_summary_old.union(patient_summary_new)\n",
    "patient_summary.write.parquet('/home/shared/yikuan/HiBEHRT/data/HF/1985_2005/test/HF_1985_2015.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# format training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diag: StructType(List(StructField(patid,StringType,true),StructField(eventdate,DateType,true),StructField(code,StringType,true)))\n",
      "med: StructType(List(StructField(patid,StringType,true),StructField(eventdate,DateType,true),StructField(code,StringType,true)))\n",
      "procedure: StructType(List(StructField(patid,StringType,true),StructField(code,StringType,true),StructField(eventdate,DateType,true)))\n",
      "test: StructType(List(StructField(patid,StringType,true),StructField(eventdate,DateType,true),StructField(code,StringType,true)))\n",
      "bmi: StructType(List(StructField(patid,StringType,true),StructField(eventdate,DateType,true),StructField(code,StringType,true)))\n",
      "bp_low: StructType(List(StructField(patid,StringType,true),StructField(eventdate,DateType,true),StructField(code,StringType,true)))\n",
      "bp_high: StructType(List(StructField(patid,StringType,true),StructField(eventdate,DateType,true),StructField(code,StringType,true)))\n",
      "smoke: StructType(List(StructField(patid,StringType,true),StructField(eventdate,DateType,true),StructField(code,StringType,true)))\n",
      "alcohol: StructType(List(StructField(patid,StringType,true),StructField(eventdate,DateType,true),StructField(code,StringType,true)))\n"
     ]
    }
   ],
   "source": [
    "# read representation list \n",
    "rep_pat = read_parquet(spark.sqlContext, '/home/shared/yikuan/HiBEHRT/data/pat_downstrean.parquet')\n",
    "rep_pat = rename_col(rep_pat, 'patid', 'patid_eligible')\n",
    "\n",
    "start_year = 1985\n",
    "end_year = 2015\n",
    "\n",
    "# diagnose\n",
    "diag = read_parquet(spark.sqlContext, '/home/shared/yikuan/HiBEHRT/data/diagnoses.parquet')\n",
    "diag = check_time(diag, 'eventdate', time_a=start_year, time_b=end_year)\n",
    "diag = diag.join(rep_pat, diag.patid==rep_pat.patid_eligible, 'left')\n",
    "diag = diag.filter(F.col('patid_eligible').isNotNull()).drop('patid_eligible')\n",
    "\n",
    "def select_diagnose(diagnoses):\n",
    "    diagnoses_icd = diagnoses.filter(F.col('type')=='icd')\\\n",
    "                    .withColumn('first', F.col('code').substr(0,1))\\\n",
    "                    .filter(F.col('first').isin(*['Z','V','R','U','X','Y']) == False)\\\n",
    "                    .select(['patid', 'eventdate', 'code', 'type', 'source'])\n",
    "    diagnoses_read = diagnoses.filter(F.col('type')=='read')\\\n",
    "                .withColumn('first', F.col('code').substr(0,1))\\\n",
    "                .filter(F.col('first').isin(*['0', '1', '2', '3', '4', '5', '6', '7','8', '9', 'Z', 'U']) == False)\\\n",
    "                .select(['patid', 'eventdate', 'code', 'type', 'source'])\n",
    "    diagnoses = diagnoses_icd.union(diagnoses_read)    \n",
    "    diagnoses = diagnoses.select(['patid', 'eventdate', 'code'])\n",
    "    return diagnoses\n",
    "\n",
    "diag = select_diagnose(diag) # patid, event date, code\n",
    "\n",
    "print('diag:',diag.schema)\n",
    "\n",
    "# medication\n",
    "med = read_parquet(spark.sqlContext, '/home/shared/yikuan/HiBEHRT/data/medication.parquet/')\n",
    "med = check_time(med, 'eventdate', time_a=start_year, time_b=end_year)\n",
    "med = med.join(rep_pat, med.patid==rep_pat.patid_eligible, 'left')\n",
    "med = med.filter(F.col('patid_eligible').isNotNull()).drop('patid_eligible') # patid, event date, code\n",
    "\n",
    "print('med:',med.schema)\n",
    "\n",
    "# hes procedure\n",
    "procedure = read_parquet(spark.sqlContext, '/home/shared/yikuan/HiBEHRT/data/hes_procedure.parquet/')\n",
    "procedure = check_time(procedure, 'eventdate', time_a=start_year, time_b=end_year)\n",
    "procedure = procedure.join(rep_pat, procedure.patid==rep_pat.patid_eligible, 'left')\n",
    "procedure = procedure.filter(F.col('patid_eligible').isNotNull()).drop('patid_eligible')\n",
    "procedure = rename_col(procedure, 'OPCS', 'code') # patid, event date, code\n",
    "\n",
    "print('procedure:',procedure.schema)\n",
    "\n",
    "# cprd test\n",
    "test = read_parquet(spark.sqlContext, '/home/shared/yikuan/HiBEHRT/data/cprd_test.parquet/')\n",
    "test = check_time(test, 'eventdate', time_a=start_year, time_b=end_year)\n",
    "test = test.join(rep_pat, test.patid==rep_pat.patid_eligible, 'left')\n",
    "test = test.filter(F.col('patid_eligible').isNotNull()).drop('patid_eligible').select(['patid', 'eventdate', 'enttype'])\n",
    "test = rename_col(test, 'enttype', 'code')\n",
    "\n",
    "print('test:',test.schema)\n",
    "\n",
    "# bmi\n",
    "bmi = read_parquet(spark.sqlContext, '/home/shared/yikuan/HiBEHRT/data/bmi.parquet/')\n",
    "bmi = check_time(bmi, 'eventdate', time_a=start_year, time_b=end_year)\n",
    "bmi = bmi.join(rep_pat, bmi.patid==rep_pat.patid_eligible, 'left')\n",
    "bmi = bmi.filter(F.col('patid_eligible').isNotNull()).drop('patid_eligible')\n",
    "\n",
    "round_bmi = F.udf(lambda x: int(x//1))\n",
    "bmi = bmi.withColumn('BMI', round_bmi('BMI'))\n",
    "bmi = rename_col(bmi, 'BMI', 'code')\n",
    "\n",
    "print('bmi:',bmi.schema)\n",
    "\n",
    "# bp_low\n",
    "bp_low = read_parquet(spark.sqlContext, '/home/shared/yikuan/HiBEHRT/data/bp_low.parquet/')\n",
    "bp_low = check_time(bp_low, 'eventdate', time_a=start_year, time_b=end_year)\n",
    "bp_low = bp_low.join(rep_pat, bp_low.patid==rep_pat.patid_eligible, 'left')\n",
    "bp_low = bp_low.filter(F.col('patid_eligible').isNotNull()).drop('patid_eligible')\n",
    "\n",
    "round_bp = F.udf(lambda x: int(x//5))\n",
    "bp_low = bp_low.withColumn('bp_low', round_bp('bp_low'))\n",
    "bp_low = rename_col(bp_low, 'bp_low', 'code')\n",
    "\n",
    "print('bp_low:',bp_low.schema)\n",
    "\n",
    "# bp_high\n",
    "bp_high = read_parquet(spark.sqlContext, '/home/shared/yikuan/HiBEHRT/data/bp_high.parquet/')\n",
    "bp_high = check_time(bp_high, 'eventdate', time_a=start_year, time_b=end_year)\n",
    "bp_high = bp_high.join(rep_pat, bp_high.patid==rep_pat.patid_eligible, 'left')\n",
    "bp_high = bp_high.filter(F.col('patid_eligible').isNotNull()).drop('patid_eligible')\n",
    "\n",
    "round_bp = F.udf(lambda x: int(x//5))\n",
    "bp_high = bp_high.withColumn('bp_high', round_bp('bp_high'))\n",
    "bp_high = rename_col(bp_high, 'bp_high', 'code')\n",
    "\n",
    "print('bp_high:', bp_high.schema)\n",
    "\n",
    "# smoke\n",
    "smoke = read_parquet(spark.sqlContext, '/home/shared/yikuan/HiBEHRT/data/smoke.parquet/')\n",
    "smoke = check_time(smoke, 'eventdate', time_a=start_year, time_b=end_year)\n",
    "smoke = smoke.join(rep_pat, smoke.patid==rep_pat.patid_eligible, 'left')\n",
    "smoke = smoke.filter(F.col('patid_eligible').isNotNull()).drop('patid_eligible')\n",
    "smoke = rename_col(smoke, 'smoke', 'code')\n",
    "\n",
    "print('smoke:',smoke.schema)\n",
    "\n",
    "# alcohol\n",
    "alcohol = read_parquet(spark.sqlContext, '/home/shared/yikuan/HiBEHRT/data/alcohol.parquet/')\n",
    "alcohol = check_time(alcohol, 'eventdate', time_a=start_year, time_b=end_year)\n",
    "alcohol = alcohol.join(rep_pat, alcohol.patid==rep_pat.patid_eligible, 'left')\n",
    "alcohol = alcohol.filter(F.col('patid_eligible').isNotNull()).drop('patid_eligible')\n",
    "alcohol = rename_col(alcohol, 'alcohol', 'code')\n",
    "\n",
    "print('alcohol:',alcohol.schema)\n",
    "\n",
    "# add signature for each mordality\n",
    "diag = diag.withColumn('code', F.concat(F.lit('DIA'), diag['code']))\n",
    "med = med.withColumn('code', F.concat(F.lit('MED'), med['code']))\n",
    "procedure = procedure.withColumn('code', F.concat(F.lit('PRO'), procedure['code']))\n",
    "test = test.withColumn('code', F.concat(F.lit('TES'), test['code']))\n",
    "bmi = bmi.withColumn('code', F.concat(F.lit('BMI'), bmi['code']))\n",
    "bp_low = bp_low.withColumn('code', F.concat(F.lit('BPL'), bp_low['code']))\n",
    "bp_high = bp_high.withColumn('code', F.concat(F.lit('BPH'), bp_high['code']))\n",
    "smoke = smoke.withColumn('code', F.concat(F.lit('SMO'), smoke['code']))\n",
    "alcohol = alcohol.withColumn('code', F.concat(F.lit('ALC'), alcohol['code']))\n",
    "\n",
    "def format_age(path_to_demo, data):\n",
    "    demographic = Patient(read_txt(spark.sc, spark.sqlContext, path=path_to_demo)) \\\n",
    "    .accept_flag().yob_calibration().cvt_crd2date().cvt_tod2date().cvt_deathdate2date().get_pracid().drop('accept')\\\n",
    "    .select(['patid', 'yob'])\n",
    "    \n",
    "    data = data.join(demographic, data.patid==demographic.patid, 'left').drop(demographic.patid)\n",
    "    data= EHR(data).cal_age('eventdate', 'yob', year=True, name='age').select(['patid', 'eventdate', 'code', 'age'])\n",
    "    return data\n",
    "\n",
    "diag = format_age('/home/workspace/datasets/cprd/cuts/02_cprd2015/1_RawData/Patient', diag)\n",
    "med = format_age('/home/workspace/datasets/cprd/cuts/02_cprd2015/1_RawData/Patient', med)\n",
    "procedure = format_age('/home/workspace/datasets/cprd/cuts/02_cprd2015/1_RawData/Patient', procedure)\n",
    "test = format_age('/home/workspace/datasets/cprd/cuts/02_cprd2015/1_RawData/Patient', test)\n",
    "bmi = format_age('/home/workspace/datasets/cprd/cuts/02_cprd2015/1_RawData/Patient', bmi)\n",
    "bp_low = format_age('/home/workspace/datasets/cprd/cuts/02_cprd2015/1_RawData/Patient', bp_low)\n",
    "bp_high = format_age('/home/workspace/datasets/cprd/cuts/02_cprd2015/1_RawData/Patient', bp_high)\n",
    "smoke = format_age('/home/workspace/datasets/cprd/cuts/02_cprd2015/1_RawData/Patient', smoke)\n",
    "alcohol = format_age('/home/workspace/datasets/cprd/cuts/02_cprd2015/1_RawData/Patient', alcohol)\n",
    "\n",
    "# ensemble\n",
    "records = diag.union(med).union(procedure).union(test).union(bmi).union(bp_low).union(bp_high).union(smoke).union(alcohol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_ref_df = read_parquet(spark.sqlContext, '/home/shared/yikuan/HiBEHRT/data/HF/1985_2005/test/HF_1985_2015.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeDiff = (F.unix_timestamp('lastDate', \"yyyy-MM-dd\") - F.unix_timestamp('firstDate', \"yyyy-MM-dd\"))\n",
    "hf_ref = hf_ref_df.withColumn(\"duration\", timeDiff).withColumn('duration', (F.col('duration')/3600/24).cast('integer'))\n",
    "\n",
    "records = records.join(hf_ref, records.patid==hf_ref.patid, 'left').drop(hf_ref.patid).dropna()\\\n",
    "            .where((F.col('eventdate') >= F.col('firstDate')) & (F.col('eventdate') <= F.col('lastDate')))\\\n",
    "            .select(['patid', 'eventdate', 'code', 'age', 'label', 'duration'])\n",
    "\n",
    "records = records.filter(F.col('duration') > 3*365).select(['patid', 'eventdate', 'code', 'age', 'label']).dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_sequence(data):\n",
    "    # group by date\n",
    "    data = data.groupby(['patid', 'eventdate']).agg(F.collect_list('code').alias('code'), F.collect_list('age').alias('age'), F.first('label').alias('label'))\n",
    "    \n",
    "    data = EHR(data).array_add_element('code', 'SEP')\n",
    "    # add extra age to fill the gap of sep\n",
    "    extract_age = F.udf(lambda x: x[0])\n",
    "    data = data.withColumn('age_temp', extract_age('age')).withColumn('age', F.concat(F.col('age'),F.array(F.col('age_temp')))).drop('age_temp')\n",
    "    \n",
    "    # sort and merge code and age\n",
    "    w = Window.partitionBy('patid').orderBy('eventdate')\n",
    "    data = data.withColumn('code',F.collect_list('code').over(w))\\\n",
    "                .withColumn('age', F.collect_list('age').over(w))\\\n",
    "                .groupBy('patid').agg(F.max('code').alias('code'), F.max('age').alias('age'), F.first('label').alias('label'))\n",
    "    data = EHR(data).array_flatten('code').array_flatten('age') # patid, code, age\n",
    "    return data\n",
    "\n",
    "records = format_sequence(records)\n",
    " \n",
    "def format_seg_and_position_code(data):\n",
    "#     \n",
    "    def seg_records(x):\n",
    "        seg_list = []\n",
    "        flag = 0\n",
    "        for each in x:\n",
    "            if each != 'SEP':\n",
    "                seg_list.append(flag)\n",
    "            else:\n",
    "                seg_list.append(flag)\n",
    "                flag = (flag + 1)%2\n",
    "                \n",
    "        return seg_list\n",
    "                \n",
    "    \n",
    "    seg = F.udf(lambda x: seg_records(x),ArrayType(StringType(),True))\n",
    "    data = data.withColumn('seg', seg('code'))\n",
    "    \n",
    "    def posi_records(x):\n",
    "        posi_list = []\n",
    "        flag = 0\n",
    "        for each in x:\n",
    "            if each != 'SEP':\n",
    "                posi_list.append(flag)\n",
    "            else:\n",
    "                posi_list.append(flag)\n",
    "                flag = flag + 1\n",
    "        return posi_list\n",
    "    \n",
    "    posi = F.udf(lambda x: posi_records(x),ArrayType(StringType(),True))\n",
    "    data = data.withColumn('position', posi('code'))\n",
    "    \n",
    "    return data\n",
    "\n",
    "records = format_seg_and_position_code(records)\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([StructField('patid', StringType(), True),\n",
    "                     StructField('code',ArrayType(StringType(),True), True),\n",
    "                     StructField('age', ArrayType(StringType(),True), True),\n",
    "                     StructField('seg', ArrayType(StringType(),True), True),\n",
    "                     StructField('position', ArrayType(StringType(),True), True),\n",
    "                     StructField('label', IntegerType(), True)\n",
    "                     ])\n",
    "\n",
    "def remove_sep(patid, code, age, seg, position, label):\n",
    "    code_list = []\n",
    "    age_list = []\n",
    "    seg_list = []\n",
    "    position_list = []\n",
    "    \n",
    "    for i in range(len(code)):\n",
    "        if code[i]!='SEP':\n",
    "            code_list.append(code[i])\n",
    "            age_list.append(age[i])\n",
    "            seg_list.append(seg[i])\n",
    "            position_list.append(position[i])\n",
    "    return patid, code, age, seg, position, label\n",
    "        \n",
    "\n",
    "test_udf = F.udf(remove_sep, schema)\n",
    "records = records.select(test_udf('patid', 'code', 'age', 'seg', 'position', 'label').alias(\"test\"))\n",
    "records = records.select(\"test.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1985-2005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = records.randomSplit([0.5,0.5])\n",
    "train = final[0]\n",
    "validation = final[1]\n",
    "\n",
    "validation = validation.randomSplit([0.4, 0.6])\n",
    "tune = validation[0]\n",
    "valid = validation[1]\n",
    "\n",
    "train.write.parquet('/home/shared/yikuan/HiBEHRT/data/HF/1985_2005/test/train.parquet')\n",
    "tune.write.parquet('/home/shared/yikuan/HiBEHRT/data/HF/1985_2005/test/tune.parquet')\n",
    "valid.write.parquet('/home/shared/yikuan/HiBEHRT/data/HF/1985_2005/test/valid.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1985 - 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_list_train = read_parquet(spark.sqlContext, '/home/shared/yikuan/HiBEHRT/data/HF/1985_2005/test/train.parquet').select(['patid'])\n",
    "pat_list_tune = read_parquet(spark.sqlContext, '/home/shared/yikuan/HiBEHRT/data/HF/1985_2005/test/tune.parquet').select(['patid'])\n",
    "\n",
    "pat_list = pat_list_train.union(pat_list_tune)\n",
    "pat_list = rename_col(pat_list, 'patid', 'patid_temp')\n",
    "\n",
    "records = records.join(pat_list, records.patid==pat_list.patid_temp, 'left').filter(F.col('patid_temp').isNull()).drop('patid_temp')\n",
    "valid.write.parquet('/home/shared/yikuan/HiBEHRT/data/HF/1985_2015/test/valid.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py3)",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
